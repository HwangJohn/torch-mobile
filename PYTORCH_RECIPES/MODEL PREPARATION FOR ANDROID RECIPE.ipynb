{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "favorite-wichita",
   "metadata": {},
   "source": [
    "This recipe demonstrates how to prepare a PyTorch MobileNet v2 image classification model for Android apps, and how to set up Android projects to use the mobile-ready model file.\n",
    "\n",
    "## Introduction\n",
    "After a PyTorch model is trained or a pre-trained model is made available, it is normally not ready to be used in mobile apps yet. It needs to be quantized (see the Quantization Recipe), converted to TorchScript so Android apps can load it, and optimized for mobile apps. Furthermore, Android apps need to be set up correctly to enable the use of PyTorch Mobile libraries, before they can load and use the model for inference.\n",
    "\n",
    "## Pre-requisites\n",
    "PyTorch 1.6.0 or 1.7.0\n",
    "\n",
    "torchvision 0.6.0 or 0.7.0\n",
    "\n",
    "Android Studio 3.5.1 or above with NDK installed\n",
    "\n",
    "## Steps\n",
    "1. Get Pretrained and Quantized MobileNet v2 Model\n",
    "To get the MobileNet v2 quantized model, simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "breeding-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/torch/lib/python3.7/site-packages/torch/quantization/observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n",
      "/usr/local/anaconda3/envs/torch/lib/python3.7/site-packages/torch/quantization/observer.py:243: UserWarning: must run observer before calling calculate_qparams.                                        Returning default scale and zero point \n",
      "  Returning default scale and zero point \"\n",
      "Downloading: \"https://download.pytorch.org/models/quantized/mobilenet_v2_qnnpack_37f702c5.pth\" to /Users/john/.cache/torch/hub/checkpoints/mobilenet_v2_qnnpack_37f702c5.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4304ff15f1754d168c661aba118d5f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/3.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "model_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-colony",
   "metadata": {},
   "source": [
    "## 2. Script and Optimize the Model for Mobile Apps\n",
    "Use either the script or trace method to convert the quantized model to the TorchScript format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opposed-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "torchscript_model = torch.jit.trace(model_quantized, dummy_input)\n",
    "\n",
    "# or\n",
    "# torchscript_model = torch.jit.script(model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-methodology",
   "metadata": {},
   "source": [
    "Then optimize the TorchScript formatted model for mobile and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certified-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "torch.jit.save(torchscript_model_optimized, \"mobilenetv2_quantized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-found",
   "metadata": {},
   "source": [
    "With the total 7 or 8 (depending on if the script or trace method is called to get the TorchScript format of the model) lines of code in the two steps above, we have a model ready to be added to mobile apps.\n",
    "\n",
    "## 3. Add the Model and PyTorch Library on Android\n",
    "In your current or a new Android Studio project, open the build.gradle file, and add the following two lines (the second one is required only if you plan to use a TorchVision model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these in gradle.build\n",
    "\n",
    "implementation 'org.pytorch:pytorch_android:1.6.0'\n",
    "implementation 'org.pytorch:pytorch_android_torchvision:1.6.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-slope",
   "metadata": {},
   "source": [
    "Drag and drop the model file mobilenetv2_quantized.pt to your project’s assets folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-offering",
   "metadata": {},
   "source": [
    "That’s it! Now you can build your Android app with the PyTorch library and the model ready to use. To actually write code to use the model, refer to the PyTorch Mobile Android Quickstart with a HelloWorld Example and Android Hackathon Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-posting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
